---
title: "2_known_prep_analysis"
author: "Ajay Prakash Nair"
date: "2024-11-14"
output: html_document
---


```{r setup, include=FALSE}
# Set CRAN mirror
options(repos = c(CRAN = "https://cran.rstudio.com/"))

# Other setup configurations
knitr::opts_chunk$set(echo = TRUE)
```

# Requirements

```{r requirements, results='hide'}
requirements=c("summarytools", "pROC", "glmnetUtils", "dplyr", "car", "effects", "gridExtra", "grid", "MASS","e1071", "mgcv", "caret", "gridExtra", "effects", "nnet", "ggplot2", "reshape2", "MASS")

for (req in requirements){
  if (!require(req, character.only = TRUE)){
      install.packages(req)
  }
}
```

# Analysis Description

This analysis focuses on building and evaluating predictive models using logistic regression, Linear Discriminant Analysis and Naive Bayes for the dataset. The dataset comprises training and testing data that was splitted in the previous stage.The objective is to assess the predictive performance of the models and evaluate their ability to classify outcomes accurately.

# Loading data

```{r}
train_data <- read.csv("./training/training.csv")
cat("Training data dimensions:", dim(train_data), "\n")

test_data <- read.csv("./testing/test.csv")
cat("Testing data dimensions:", dim(test_data), "\n")
```

# Logistic Regression

The logistic regression analysis involves two main stages: model building and evaluation. In the model-building stage, logistic regression models are trained using the training dataset. Two models are considered: one with all variables and another with only relevant variables. The significance of each predictor variable is assessed based on the model summary and p-values.

```{r}
# Relevel reference level as "Draw"
train_data$FTR <- relevel(factor(train_data$FTR), ref = "Draw")

# Fit initial model
multinom_model <- multinom(FTR ~ HTHG + HTAG + HTR + HS + AS + HST + AST + HF + AF + HC + AC + HY + AY + HR + AR, data = train_data)

summary(multinom_model)
```

### Key Findings:

- **Half-Time Goals**:
  - **HTHG** (Half-Time Home Team Goals) significantly influences match outcomes and is especially important for predicting **Home Wins**.
  - **HTAG** (Half-Time Away Team Goals) is more important for predicting **Away Wins**.

- **Half-Time Result (HTR)**:
  - **HTR** (Half-Time Result) plays an important role in determining match outcomes, particularly predicting **Home Wins** and **Away Wins** based on the halftime result.

- **Shots on Target**:
  - **HST** (Home Shots on Target) is significant for predicting **Home Wins**, where more shots on target decrease the likelihood of an **Away Win**.
  - **AST** (Away Shots on Target) is significant for predicting **Away Wins**, with more shots on target increasing the likelihood of an **Away Win**.

- **Red Cards**:
  - **HR** (Home Red Cards) has a significant positive effect on **Home Wins**. More home red cards increase the likelihood of a **Home Win**.
  
  - **AR** (Away Red Cards) has a minor effect on **Home Wins**, suggesting it has a slight influence on match outcomes.

- **Other Match Statistics**:
  - Most other match statistics, such as **Fouls**, **Yellow Cards**, and **Corners**, show smaller or no significant effects on match outcomes.

### Checking Multicollinearity

```{r}
vif(multinom_model)
```

The HTR (Half-Time Result) variable has a high VIF of 49.97. Also, HS and AS have a VIF of more than 16 , suggesting significant multicollinearity with other predictors in the model. So, lets check mullticollinearity by removing it.

```{r}
multinom_model.clean <- update(multinom_model, .~.-HTR -HS -AS)
vif(multinom_model.clean)

```

After removing HTR, HS, and AS from the model, the VIF results show improved multicollinearity for the remaining variables except AF and HF. So, removing AF and HF too.

```{r}
multinom_model.clean <- update(multinom_model.clean, .~.-HF -AF)
vif(multinom_model.clean)

```

Let’s see the summary of the updated cleaned model.

```{r}
summary(multinom_model.clean)
```

Compare the AIC for the two models.

```{r}
AIC(multinom_model, multinom_model.clean) %>% arrange(AIC)
```

The cleaned model appears to have a higher AIC thus up to now we prefer the full initial model. Lower AIC indicates a better fit, so multinom_model is the preferred model between the two in terms of fit. multinom_model.clean has fewer predictors (22 degrees of freedom vs 34) but results in a higher AIC, suggesting it may have sacrificed model complexity without improving fit sufficiently.

## Interaction Terms

We are adding the following interaction terms : 
HTHG × HTAG: Home half-time goals × Away half-time goals
HST × AST: Home shots on target × Away shots on target
HR × AR: Home red cards × Away red cards
HY × AY: Home yellow cards × Away yellow cards

```{r}
multinom_model.inter <- update(multinom_model, 
                         . ~ . + HTHG:HTAG + HST:AST + HR:AR + HY:AY)

summary(multinom_model.inter)

```

HST:AST, HY:AY appear to be not so much significant. So removing it.

```{r}
multinom_model.inter <- update(multinom_model.inter, . ~ . -HST:AST -HY:AY)
summary(multinom_model.inter)
```

Let's check the AIC

```{r}
AIC(multinom_model, multinom_model.clean, multinom_model.inter) %>% arrange(AIC)
```

By looking at the AIC the best model remain the initial full model. Now let’s try adding interaction terms to the clean model. Now let’s add the following interactions:

HTHG × HTAG: Home half-time goals × Away half-time goals
HST × AST: Home shots on target × Away shots on target
HR × AR: Home red cards × Away red cards
HY × AY: Home yellow cards × Away yellow cards

```{r}
multinom_model.clean.inter <- update(multinom_model.clean, 
                         . ~ . + HTHG:HTAG + HST:AST + HR:AR + HY:AY)

summary(multinom_model.clean.inter)

```

```{r}
AIC(multinom_model, multinom_model.inter, multinom_model.clean, multinom_model.clean.inter) %>% arrange(AIC)
```

And it seems like that the full model is just slightly better with respect to the full model with interaction terms.

## Model Interpretation

We have decided to interpret the model multinom_model based on its lower AIC value (5860.762), which suggests that it provides the best balance between fit and complexity.

To begin the interpretation, we recall the model’s summary:

```{r}
summary(multinom_model)
```

```{r}

# Plot for HTHG (Home Team Home Goals)
a <- effect("HTHG", multinom_model)
plot(a, rescale.axis = FALSE, ylab = "Probability of FTR")

# Plot for HTAG (Home Team Away Goals)
b <- effect("HTAG", multinom_model)
plot(b, rescale.axis = FALSE, ylab = "Probability of FTR")

# Plot for HST (Home Team Shots)
c <- effect("HST", multinom_model)
plot(c, rescale.axis = FALSE, ylab = "Probability of FTR")

# Plot for AST (Away Team Shots)
d <- effect("AST", multinom_model)
plot(d, rescale.axis = FALSE, ylab = "Probability of FTR")

```

As we can see, if Half Time Home Team Goals (HTHG) are greater than 3, the probability of the home team winning is 1; if Half Time Away Goals (HTAG) are greater than 2, the probability of the away team winning is more than 50%; if the Home Team Shots on Target (HST) are greater than 10, the probability of the home team winning is high; and if the Away Team Shots on Target (AST) are greater than 10, the probability of the away team winning is high.

## Model Comparison

Let’s compare all the models done so far in terms of prediction power to be able to compare them with other types of models:

```{r}
test_data <- read.csv("testing/test.csv")
```

```{r}
plot_roc <- function(test_data, prob_matrix, model_name) {
  roc_list <- lapply(1:ncol(prob_matrix), function(i) {
    roc(test_data$FTR == colnames(prob_matrix)[i], prob_matrix[, i])
  })
  
  auc_values <- sapply(roc_list, function(r) auc(r))
  class_names <- colnames(prob_matrix)
  
  for (i in 1:length(roc_list)) {
    class_name <- class_names[i]
    class_roc <- roc_list[[i]]
    
    roc_data <- data.frame(
      Specificity = 1 - class_roc$specificities,
      Sensitivity = class_roc$sensitivities
    )
    
    p <- ggplot(data = roc_data, aes(x = Specificity, y = Sensitivity)) +
      geom_line(size = 1, color = "black") + 
      geom_point(color = "black") +  
      labs(
        title = paste("ROC Curve for", model_name, "-", class_name),
        x = "1 - Specificity",
        y = "Sensitivity"
      ) +
      theme_minimal() +
      theme(plot.title = element_text(size = 14, face = "bold"),
            plot.subtitle = element_text(size = 12)) +
      annotate("text", x = 0.5, y = 0.1, 
               label = paste("AUC:", round(auc_values[i], 3)), 
               size = 5, hjust = 0.5)
    
    print(p)
  }
}


# Compute probabilities for each model
prob_multinom_model <- predict(multinom_model, test_data, type = "prob")
prob_multinom_model.inter <- predict(multinom_model.inter, test_data, type = "prob")
prob_multinom_model.clean <- predict(multinom_model.clean, test_data, type = "prob")
prob_multinom_model.clean.inter <- predict(multinom_model.clean.inter, test_data, type = "prob")

# Plot ROC curves for each model
plot_roc(test_data, prob_multinom_model, "Multinom Model")
plot_roc(test_data, prob_multinom_model.inter, "Multinom Model with Interaction")
plot_roc(test_data, prob_multinom_model.clean, "Cleaned Multinom Model")
plot_roc(test_data, prob_multinom_model.clean.inter, "Cleaned Multinom Model with Interaction")

```

By the plots we can see that the Area Under the Curve (AUC) computed by the ROC curves gives almost the same results.

Now we extract from each roc object the best threshold in order to use it during predictions.

```{r}
# Compute individual ROC curves for each class
roc_list_full <- lapply(1:ncol(prob_multinom_model), function(i) {
  roc(test_data$FTR == colnames(prob_multinom_model)[i], prob_multinom_model[, i])
})
roc_list_full_inter <- lapply(1:ncol(prob_multinom_model.inter), function(i) {
  roc(test_data$FTR == colnames(prob_multinom_model.inter)[i], prob_multinom_model.inter[, i])
})
roc_list_clean <- lapply(1:ncol(prob_multinom_model.clean), function(i) {
  roc(test_data$FTR == colnames(prob_multinom_model.clean)[i], prob_multinom_model.clean[, i])
})
roc_list_clean_inter <- lapply(1:ncol(prob_multinom_model.clean.inter), function(i) {
  roc(test_data$FTR == colnames(prob_multinom_model.clean.inter)[i], prob_multinom_model.clean.inter[, i])
})

# Extract metrics for each ROC curve
extract_metrics <- function(roc_list) {
  lapply(roc_list, function(r) {
    # Get the best coordinates (threshold, specificity, sensitivity, accuracy)
    coords(r, x = "best", ret = c("threshold", "specificity", "sensitivity", "accuracy"))
  })
}

# Extract metrics for each model
roc_metrics_full <- extract_metrics(roc_list_full)
roc_metrics_full_inter <- extract_metrics(roc_list_full_inter)
roc_metrics_clean <- extract_metrics(roc_list_clean)
roc_metrics_clean_inter <- extract_metrics(roc_list_clean_inter)

```

Rename the single row for each dataframe and concatenate all in a single one.

```{r}
# Combine metrics for each model into a data frame
combine_metrics <- function(roc_metrics_list) {
  metrics <- do.call(rbind, roc_metrics_list)
  rownames(metrics) <- colnames(prob_multinom_model)
  return(metrics)
}

# Combine metrics for all models
roc_metrics_full <- combine_metrics(roc_metrics_full)
roc_metrics_full_inter <- combine_metrics(roc_metrics_full_inter)
roc_metrics_clean <- combine_metrics(roc_metrics_clean)
roc_metrics_clean_inter <- combine_metrics(roc_metrics_clean_inter)

```

Now we make a comparisons result: 

```{r}
# Compute average metrics (across classes) for each model
average_metrics <- function(metrics) {
  colMeans(metrics, na.rm = TRUE)
}

# Calculate average metrics for each model
average_metrics_full <- average_metrics(roc_metrics_full)
average_metrics_full_inter <- average_metrics(roc_metrics_full_inter)
average_metrics_clean <- average_metrics(roc_metrics_clean)
average_metrics_clean_inter <- average_metrics(roc_metrics_clean_inter)


# Combine into a single data frame with model names and metrics
comparison_df <- data.frame(
  Model = rep(c("Full Model", "Full Interaction Model", "Cleaned Model", "Cleaned Interaction Model"), each = 3),
  Class = rep(c("Away Win", "Draw", "Home Win"), times = 4),
  Accuracy = c(average_metrics_full["accuracy"], 
               average_metrics_full_inter["accuracy"],
               average_metrics_clean["accuracy"], 
               average_metrics_clean_inter["accuracy"]),
  Sensitivity = c(average_metrics_full["sensitivity"], 
                  average_metrics_full_inter["sensitivity"],
                  average_metrics_clean["sensitivity"], 
                  average_metrics_clean_inter["sensitivity"]),
  Specificity = c(average_metrics_full["specificity"], 
                  average_metrics_full_inter["specificity"],
                  average_metrics_clean["specificity"], 
                  average_metrics_clean_inter["specificity"])
)

# Print the comparison data frame
print(comparison_df)

```

The Full Model is best for accuracy, the Full Interaction Model excels in sensitivity, and the Cleaned Model performs best in specificity. We can see that the response is unbalanced.

### Confusion Matrixes

Let’s have a quick look on the confusion matrix for the four model that we have specified.

```{r}
test_data$FTR <- factor(test_data$FTR, levels = c("Away Win", "Draw", "Home Win"))

# Predict class labels for each model using the best thresholds
pred_multinom_model <- factor(predict(multinom_model, test_data, type = "class"), levels = c("Away Win", "Draw", "Home Win"))
pred_multinom_model.inter <- factor(predict(multinom_model.inter, test_data, type = "class"), levels = c("Away Win", "Draw", "Home Win"))
pred_multinom_model.clean <- factor(predict(multinom_model.clean, test_data, type = "class"), levels = c("Away Win", "Draw", "Home Win"))
pred_multinom_model.clean.inter <- factor(predict(multinom_model.clean.inter, test_data, type = "class"), levels = c("Away Win", "Draw", "Home Win"))

# Generate confusion matrices for each model
conf_matrix_multinom_model <- confusionMatrix(pred_multinom_model, test_data$FTR)
conf_matrix_multinom_model.inter <- confusionMatrix(pred_multinom_model.inter, test_data$FTR)
conf_matrix_multinom_model.clean <- confusionMatrix(pred_multinom_model.clean, test_data$FTR)
conf_matrix_multinom_model.clean.inter <- confusionMatrix(pred_multinom_model.clean.inter, test_data$FTR)
```

1. Full Model


```{r}
print(conf_matrix_multinom_model)
```

2. Full Interaction Model

```{r}
print(conf_matrix_multinom_model.inter)
```

3. Cleaned Model


```{r}
print(conf_matrix_multinom_model.clean)
```

4. Cleaned Interaction Model


```{r}
print(conf_matrix_multinom_model.clean.inter)
```

## Model Selection

### Stepwise Model

Now let’s apply automatic model selection to identity the best model.

```{r}
full_model <- multinom(FTR ~ HTHG + HTAG + HTR + HS + AS + HST + AST + HF + AF + HC + AC + HY + AY + HR + AR, data = test_data)

stepwise_model <- stepAIC(full_model, direction = "both", trace = 1)

summary(stepwise_model)
```

```{r}
# Calculate AIC for each model
aic_full <- AIC(multinom_model)
aic_full_inter <- AIC(multinom_model.inter)
aic_clean <- AIC(multinom_model.clean)
aic_clean_inter <- AIC(multinom_model.clean.inter)
aic_stepwise <- AIC(stepwise_model)

aic_results <- data.frame(
  Model = c("Full Model", "Interaction Full Model", "Cleaned Model", "Interaction Cleaned Model", "Stepwise Model"),
  AIC = c(aic_full, aic_full_inter, aic_clean, aic_clean_inter, aic_stepwise)
)

aic_results_sorted <- aic_results %>% arrange(AIC)

print(aic_results_sorted)
```

In terms of AIC the “Step model” is better.

```{r}
prob_stepwise_model <- predict(stepwise_model, test_data, type = "prob")

plot_roc(test_data, prob_stepwise_model, "Stepwise Model")
```

Confusion Matrix for stepwise model

```{r}
pred_stepwise <- predict(stepwise_model, test_data, type = "class")
pred_stepwise <- factor(pred_stepwise, levels = levels(test_data$FTR))

conf_matrix_stepwise <- confusionMatrix(pred_stepwise, test_data$FTR)

print(conf_matrix_stepwise)
```

Let's compare it with other models

```{r}
specificity_awaywin <- conf_matrix_stepwise$byClass["Class: Away Win", "Specificity"]
sensitivity_awaywin <- conf_matrix_stepwise$byClass["Class: Away Win", "Sensitivity"]

specificity_draw <- conf_matrix_stepwise$byClass["Class: Draw", "Specificity"]
sensitivity_draw <- conf_matrix_stepwise$byClass["Class: Draw", "Sensitivity"]

specificity_homewin <- conf_matrix_stepwise$byClass["Class: Home Win", "Specificity"]
sensitivity_homewin <- conf_matrix_stepwise$byClass["Class: Home Win", "Sensitivity"]

accuracy_stepwise <- conf_matrix_stepwise$overall["Accuracy"]

metrics_stepwise <- data.frame(
  Model = rep("Stepwise Model", 3),
  Class = c("Away Win", "Draw", "Home Win"),
  Accuracy = c(accuracy_stepwise, accuracy_stepwise, accuracy_stepwise),  # Assuming accuracy is the same for all classes
  Sensitivity = c(sensitivity_awaywin, sensitivity_draw, sensitivity_homewin),
  Specificity = c(specificity_awaywin, specificity_draw, specificity_homewin),
  stringsAsFactors = FALSE  # Ensure strings are not converted to factors
)

comparison_df <- rbind(
  comparison_df,  
  metrics_stepwise
)

print(comparison_df)
```

### Lasso Regression

```{r}
# Prepare the data
x <- model.matrix(FTR ~ ., data = train_data)[, -1]
y <- train_data$FTR  

# Scale predictors
x_scaled <- scale(x)

model_fit.lasso <- glmnet(x_scaled, y, family = "multinomial", alpha = 1)

plot(model_fit.lasso, xvar = "lambda", label = TRUE)
```

We choose lambda using cross validation:

```{r}
modelcv.lasso <- cv.glmnet(x_scaled, y, alpha = 1, family = "multinomial")

plot(modelcv.lasso)
```

The value of λ that minimizes the ridge cross-validated mean square error is:

```{r}
lambda_min <- modelcv.lasso$lambda.min
lambda_min
```

We take as λ value corresponding to one standard error from the minimum of the cross-validated mean square error:


```{r}
lambda_1se <- modelcv.lasso$lambda.1se
lambda_1se
```

Now visualize again the lasso estimates as a function of the logarithm of λ and add a vertical line corresponding to best.lambda:

```{r}
plot(model_fit.lasso, xvar = "lambda", label = TRUE)
abline(v = log(lambda_1se), lwd = 1.2, lty = "dashed")
```

These are the coefficients:

```{r}
model_bestlam.lasso.coef <- coef(model_fit.lasso, s = lambda_1se)

nonzero_coefs <- lapply(model_bestlam.lasso.coef, function(coef_matrix) {
  coef_matrix <- as.matrix(coef_matrix)
  coef_matrix[coef_matrix != 0, , drop = FALSE]
})

print(nonzero_coefs)

```

Now we apply prediction on the test set.

```{r}

x_train <- model.matrix(FTR ~ ., data = train_data)[, -1]
x_test <- model.matrix(FTR ~ ., data = test_data)[, -1]

# Align columns between training and test sets
common_columns <- intersect(colnames(x_train), colnames(x_test))
x_test <- x_test[, common_columns, drop = FALSE]                  
x_train <- x_train[, common_columns, drop = FALSE]                

# Rescale training data
x_train_scaled <- scale(x_train)

# Scale the test set using training set scaling parameters
x_test_scaled <- scale(x_test, 
                       center = attr(x_train_scaled, "scaled:center"), 
                       scale = attr(x_train_scaled, "scaled:scale"))

# Fit LASSO regression on the training set
model_fit.lasso <- glmnet(x_train_scaled, train_data$FTR, 
                          family = "multinomial", alpha = 1)

# Perform cross-validation to select lambda
modelcv.lasso <- cv.glmnet(x_train_scaled, train_data$FTR, 
                           family = "multinomial", alpha = 1)

# Get the best lambda (lambda.1se)
model_bestlam.lasso <- modelcv.lasso$lambda.1se

# Predict probabilities for the test set
pred_prob_lasso <- predict(model_fit.lasso, 
                           newx = x_test_scaled, 
                           s = model_bestlam.lasso, 
                           type = "response")

# Predict class labels for the test set
pred_class_lasso <- predict(model_fit.lasso, 
                            newx = x_test_scaled, 
                            s = model_bestlam.lasso, 
                            type = "class")

# Convert predictions to factors with the same levels as the test response variable
pred_class_lasso <- factor(pred_class_lasso, levels = levels(test_data$FTR))

```

Now, let's see the confusion matrix.

```{r}
conf_matrix_lasso <- confusionMatrix(pred_class_lasso, test_data$FTR)

print(conf_matrix_lasso)
```

Then we apply the roc function to obtain the best threshold value.

```{r}
if (!is.data.frame(pred_prob_lasso)) {
  pred_prob_lasso <- as.data.frame(pred_prob_lasso)
}
```


```{r}
colnames(pred_prob_lasso) <- levels(test_data$FTR)

test_labels <- model.matrix(~ test_data$FTR - 1)
colnames(test_labels) <- levels(test_data$FTR)

plot_roc(test_data, pred_prob_lasso, "LASSO Model")

```

```{r}
specificity_awaywin_lasso <- conf_matrix_lasso$byClass["Class: Away Win", "Specificity"]
sensitivity_awaywin_lasso <- conf_matrix_lasso$byClass["Class: Away Win", "Sensitivity"]

specificity_draw_lasso <- conf_matrix_lasso$byClass["Class: Draw", "Specificity"]
sensitivity_draw_lasso <- conf_matrix_lasso$byClass["Class: Draw", "Sensitivity"]

specificity_homewin_lasso <- conf_matrix_lasso$byClass["Class: Home Win", "Specificity"]
sensitivity_homewin_lasso <- conf_matrix_lasso$byClass["Class: Home Win", "Sensitivity"]

accuracy_lasso <- conf_matrix_lasso$overall["Accuracy"]

metrics_lasso <- data.frame(
  Model = rep("LASSO Model", 3),
  Class = c("Away Win", "Draw", "Home Win"),
  Accuracy = c(accuracy_lasso, accuracy_lasso, accuracy_lasso),  # Assuming accuracy is the same for all classes
  Sensitivity = c(sensitivity_awaywin_lasso, sensitivity_draw_lasso, sensitivity_homewin_lasso),
  Specificity = c(specificity_awaywin_lasso, specificity_draw_lasso, specificity_homewin_lasso),
  stringsAsFactors = FALSE  # Ensure strings are not converted to factors
)

comparison_df <- rbind(
  comparison_df,  
  metrics_lasso
)

print(comparison_df)
```

### Ridge logistic regression

Now we try with ridge regression. As before we use as starting point the formula with all predictors and interaction term.

```{r}
x <- model.matrix(FTR ~ ., data = train_data)[, -1] 

y <- train_data$FTR

model_fit.ridge <- glmnet(
  x, 
  y, 
  family = "multinomial", 
  alpha = 0
)

plot(model_fit.ridge, xvar = "lambda", label = TRUE)
```

Choose the best value of lambda via cross-validation.

```{r}
modelcv.ridge <- cv.glmnet(
  x, 
  y, 
  alpha = 0,  
  family = "multinomial"  
)

plot(modelcv.ridge)
```

The value of λ that minimizes the ridge cross-validated mean square error is:

```{r}
modelcv.ridge$lambda.min
```

However, we select the simplest model whose λ value is within one standard error from the minimum of the cross-validated mean square error:

```{r}
modelcv_bestlam.ridge <- modelcv.ridge$lambda.1se
modelcv_bestlam.ridge
```

Now visualize again the ridge estimates as a function of the logarithm of λ and add a vertical line corresponding to best.lambda:

```{r}
plot(model_fit.ridge, xvar = "lambda", label = TRUE)

abline(v = log(modelcv_bestlam.ridge), lwd = 1.2, lty = "dashed", col = "red")
```

These are the coefficients:

```{r}
model_bestlam.ridge.coef <- coef(model_fit.ridge, s = lambda_1se)

nonzero_coefs <- lapply(model_bestlam.ridge.coef, function(coef_matrix) {
  coef_matrix <- as.matrix(coef_matrix)
  coef_matrix[coef_matrix != 0, , drop = FALSE]
})

print(nonzero_coefs)
```

We make the prediction on the test set.

```{r}
model_bestlam.ridge.coef <- coef(model_fit.ridge, s = lambda_1se)

nonzero_coefs <- lapply(model_bestlam.ridge.coef, function(coef_matrix) {
  coef_matrix <- as.matrix(coef_matrix)
  coef_matrix[coef_matrix != 0, , drop = FALSE]
})

print(nonzero_coefs)
```

```{r}
x_train <- model.matrix(FTR ~ ., data = train_data)[, -1]
x_train_scaled <- scale(x_train)

x_test <- model.matrix(FTR ~ ., data = test_data)[, -1]

common_columns <- intersect(colnames(x_train), colnames(x_test))
x_test <- x_test[, common_columns, drop = FALSE]                  
x_train <- x_train[, common_columns, drop = FALSE]                

x_train_scaled <- scale(x_train)

x_test_scaled <- scale(x_test, 
                       center = attr(x_train_scaled, "scaled:center"), 
                       scale = attr(x_train_scaled, "scaled:scale"))

# Fit Ridge regression on the training set
model_fit.ridge <- glmnet(x_train_scaled, train_data$FTR, 
                          family = "multinomial", alpha = 0)

# Perform cross-validation to select lambda
modelcv.ridge <- cv.glmnet(x_train_scaled, train_data$FTR, 
                           family = "multinomial", alpha = 0)

# Get the best lambda (lambda.1se)
model_bestlam.ridge <- modelcv.ridge$lambda.1se

pred_class_ridge <- predict(model_fit.ridge, newx = x_test_scaled, s = model_bestlam.ridge, type = "class")

cat("Predicted classes:\n")
print(table(pred_class_ridge))

test_data$FTR <- factor(test_data$FTR)
pred_class_ridge <- factor(pred_class_ridge, levels = levels(test_data$FTR))

cat("\nOverlap in levels:\n")
print(intersect(levels(pred_class_ridge), levels(test_data$FTR)))

conf_matrix_ridge <- confusionMatrix(pred_class_ridge, test_data$FTR)

print(conf_matrix_ridge)

```

```{r}
pred_prob_ridge <- predict(model_fit.ridge, 
                           newx = x_test_scaled, 
                           s = model_bestlam.ridge, 
                           type = "response")

pred_prob_ridge <- matrix(pred_prob_ridge, ncol = length(levels(test_data$FTR)))

colnames(pred_prob_ridge) <- levels(test_data$FTR)

test_labels <- model.matrix(~ test_data$FTR - 1)
colnames(test_labels) <- levels(test_data$FTR)

plot_roc(test_data, pred_prob_ridge, "Ridge Model")
```

```{r}
specificity_awaywin_ridge <- conf_matrix_ridge$byClass["Class: Away Win", "Specificity"]
sensitivity_awaywin_ridge <- conf_matrix_ridge$byClass["Class: Away Win", "Sensitivity"]

specificity_draw_ridge <- conf_matrix_ridge$byClass["Class: Draw", "Specificity"]
sensitivity_draw_ridge <- conf_matrix_ridge$byClass["Class: Draw", "Sensitivity"]

specificity_homewin_ridge <- conf_matrix_ridge$byClass["Class: Home Win", "Specificity"]
sensitivity_homewin_ridge <- conf_matrix_ridge$byClass["Class: Home Win", "Sensitivity"]

accuracy_ridge <- conf_matrix_ridge$overall["Accuracy"]

metrics_ridge <- data.frame(
  Model = rep("Ridge Model", 3),
  Class = c("Away Win", "Draw", "Home Win"),
  Accuracy = c(accuracy_ridge, accuracy_ridge, accuracy_ridge), 
  Sensitivity = c(sensitivity_awaywin_ridge, sensitivity_draw_ridge, sensitivity_homewin_ridge),
  Specificity = c(specificity_awaywin_ridge, specificity_draw_ridge, specificity_homewin_ridge),
  stringsAsFactors = FALSE
)

comparison_df <- rbind(
  comparison_df,  
  metrics_ridge
)

print(comparison_df)
```

### Linear Discriminant Analysis

Now we are going to deal with LDA or Linear Discriminant Analysis. For this type of Generative Model we decide to again use the full model.

```{r}
x_train <- model.matrix(FTR ~ ., data = train_data)[, -1]
x_train_scaled <- scale(x_train)

x_test <- model.matrix(FTR ~ ., data = test_data)[, -1]

common_columns <- intersect(colnames(x_train), colnames(x_test))
x_test <- x_test[, common_columns, drop = FALSE]                  
x_train <- x_train[, common_columns, drop = FALSE]                

x_train_scaled <- scale(x_train)

x_test_scaled <- scale(x_test, 
                       center = attr(x_train_scaled, "scaled:center"), 
                       scale = attr(x_train_scaled, "scaled:scale"))

lda_model <- lda(FTR ~ ., data = data.frame(x_train_scaled, FTR = train_data$FTR))

```

```{r}
pred_class_lda <- predict(lda_model, newdata = data.frame(x_test_scaled))$class

cat("Predicted classes:\n")
print(table(pred_class_lda))

test_data$FTR <- factor(test_data$FTR)
pred_class_lda <- factor(pred_class_lda, levels = levels(test_data$FTR))

cat("\nOverlap in levels:\n")
print(intersect(levels(pred_class_lda), levels(test_data$FTR)))
```

```{r}
conf_matrix_lda <- confusionMatrix(pred_class_lda, test_data$FTR)

print(conf_matrix_lda)
```

```{r}
specificity_awaywin_lda <- conf_matrix_lda$byClass["Class: Away Win", "Specificity"]
sensitivity_awaywin_lda <- conf_matrix_lda$byClass["Class: Away Win", "Sensitivity"]

specificity_draw_lda <- conf_matrix_lda$byClass["Class: Draw", "Specificity"]
sensitivity_draw_lda <- conf_matrix_lda$byClass["Class: Draw", "Sensitivity"]

specificity_homewin_lda <- conf_matrix_lda$byClass["Class: Home Win", "Specificity"]
sensitivity_homewin_lda <- conf_matrix_lda$byClass["Class: Home Win", "Sensitivity"]

accuracy_lda <- conf_matrix_lda$overall["Accuracy"]

metrics_lda <- data.frame(
  Model = rep("LDA Model", 3),
  Class = c("Away Win", "Draw", "Home Win"),
  Accuracy = c(accuracy_lda, accuracy_lda, accuracy_lda), 
  Sensitivity = c(sensitivity_awaywin_lda, sensitivity_draw_lda, sensitivity_homewin_lda),
  Specificity = c(specificity_awaywin_lda, specificity_draw_lda, specificity_homewin_lda),
  stringsAsFactors = FALSE
)

comparison_df <- rbind(
  comparison_df,  
  metrics_lda
)

print(comparison_df)
```

```{r}
# Get predicted probabilities for LDA model
pred_prob_lda <- predict(lda_model, newdata = data.frame(x_test_scaled))$posterior

# Ensure the column names match the levels of the response variable
colnames(pred_prob_lda) <- levels(test_data$FTR)

# Generate true labels for ROC computation
test_labels <- model.matrix(~ test_data$FTR - 1)
colnames(test_labels) <- levels(test_data$FTR)

plot_roc(test_data, pred_prob_lda, "LDA Model")
```

### Naive Bayes

Continuing with the analysis we try Naive Bayes algorithm. In this case we can’t use a model with interaction term since Naive Bayes do not support these relations. Thus we decide to use the Full Model. 

```{r}
model_nb <- naiveBayes(FTR ~ ., data = train_data)
model_nb
```

Prediction for the test set:

```{r}
# Get predicted probabilities for Naive Bayes model
pred_prob_nb <- predict(model_nb, newdata = test_data, type = "raw")
```

Let's plot the ROC :

```{r}
# Ensure the column names match the levels of the response variable
colnames(pred_prob_nb) <- levels(test_data$FTR)

# Generate true labels for ROC computation
test_labels <- model.matrix(~ test_data$FTR - 1)
colnames(test_labels) <- levels(test_data$FTR)

plot_roc(test_data, pred_prob_nb, "Naive Bayes Model")
```

```{r}
pred_class_nb <- predict(model_nb, newdata = test_data)

test_data$FTR <- factor(test_data$FTR)
pred_class_nb <- factor(pred_class_nb, levels = levels(test_data$FTR))

conf_matrix_nb <- confusionMatrix(pred_class_nb, test_data$FTR)

print(conf_matrix_nb)
```

```{r}
specificity_awaywin_nb <- conf_matrix_nb$byClass["Class: Away Win", "Specificity"]
sensitivity_awaywin_nb <- conf_matrix_nb$byClass["Class: Away Win", "Sensitivity"]

specificity_draw_nb <- conf_matrix_nb$byClass["Class: Draw", "Specificity"]
sensitivity_draw_nb <- conf_matrix_nb$byClass["Class: Draw", "Sensitivity"]

specificity_homewin_nb <- conf_matrix_nb$byClass["Class: Home Win", "Specificity"]
sensitivity_homewin_nb <- conf_matrix_nb$byClass["Class: Home Win", "Sensitivity"]

accuracy_nb <- conf_matrix_nb$overall["Accuracy"]

metrics_nb <- data.frame(
  Model = rep("Naive Bayes", 3),
  Class = c("Away Win", "Draw", "Home Win"),
  Accuracy = c(accuracy_nb, accuracy_nb, accuracy_nb),
  Sensitivity = c(sensitivity_awaywin_nb, sensitivity_draw_nb, sensitivity_homewin_nb),
  Specificity = c(specificity_awaywin_nb, specificity_draw_nb, specificity_homewin_nb),
  stringsAsFactors = FALSE
)

comparison_df <- rbind(
  comparison_df,  
  metrics_nb
)

print(comparison_df)

```

## Conclusion

```{r}
print(comparison_df)
```

Let’s find the best model for all the metrics:

1. Accuracy

```{r}
best_models_accuracy <- comparison_df %>%
  group_by(Class) %>%
  slice(which.max(Accuracy)) %>%
  ungroup()  

print(best_models_accuracy)
```

The best model in terms of accuracy is the **Full Model** for the class "Away Win," the **Full Interaction Model** for the class "Draw," and the **Cleaned Model** for the class "Home Win."

2. Sensitivity

```{r}
best_models_sensitivity <- comparison_df %>%
  group_by(Class) %>%
  slice(which.max(Sensitivity)) %>%
  ungroup()  

print(best_models_sensitivity)
```

The best model in terms of sensitivity is the **Cleaned Interaction Model** for the class "Away Win," the **Full Model** for the class "Draw," and the **Stepwise Model** for the class "Home Win."

3. Specificity

```{r}
best_models_specificity <- comparison_df %>%
  group_by(Class) %>%
  slice(which.max(Specificity)) %>%
  ungroup()  

print(best_models_specificity)
```

The best model in terms of specificity is the **Stepwise Model** for the class "Away Win," the **LASSO Model** for the class "Draw," and the **Naive Bayes** for the class "Home Win."

However, recalling the unbalanced problem, our choice of the top three overall models is as follows:

1. **Full Model**: This model demonstrates strong performance across all classes, particularly in accuracy and sensitivity for the Away Win and Home Win classes.

2. **Full Interaction Model**: This model also performs well, especially for the Draw class, indicating its effectiveness in capturing interactions between predictors.

3. **Cleaned Model**: This model shows competitive performance, particularly for the Home Win class, suggesting that it effectively balances complexity and interpretability.
