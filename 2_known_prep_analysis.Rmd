---
title: "2_known_prep_analysis"
author: "Ajay Prakash Nair"
date: "2024-11-14"
output: html_document
---


```{r setup, include=FALSE}
# Set CRAN mirror
options(repos = c(CRAN = "https://cran.rstudio.com/"))

# Other setup configurations
knitr::opts_chunk$set(echo = TRUE)
```

# Requirements

```{r requirements, results='hide'}
requirements=c("summarytools", "pROC", "glmnetUtils", "dplyr", "car", "effects", "gridExtra", "grid", "MASS","e1071", "mgcv", "caret")

for (req in requirements){
  if (!require(req, character.only = TRUE)){
      install.packages(req)
  }
}
```

# Analysis Description

This analysis focuses on building and evaluating predictive models using logistic regression and Naive Bayes for the dataset. The dataset comprises training and testing data that was splitted in the previous stage. Logistic regression and Naive Bayes is employed to predict outcomes based on a set of predictor variables. The objective is to assess the predictive performance of the models and evaluate their ability to classify outcomes accurately.

# Loading data

```{r}
train_data <- read.csv("./training/training.csv")
cat("Training data dimensions:", dim(train_data), "\n")

test_data <- read.csv("./testing/test.csv")
cat("Testing data dimensions:", dim(test_data), "\n")
```

# Logistic Regression

The logistic regression analysis involves two main stages: model building and evaluation. In the model-building stage, logistic regression models are trained using the training dataset. Two models are considered: one with all variables and another with only relevant variables. The significance of each predictor variable is assessed based on the model summary and p-values.

```{r}
# Relevel reference level as "Draw"
train_data$FTR <- relevel(factor(train_data$FTR), ref = "Draw")

library(nnet)

# Fit initial model
multinom_model <- multinom(FTR ~ HTHG + HTAG + HTR + HS + AS + HST + AST + HF + AF + HC + AC + HY + AY + HR + AR, data = train_data)

# Summary
summary(multinom_model)

```

### Key Findings:

- **Half-Time Goals**:
  - **HTHG** (Half-Time Home Team Goals) significantly influences match outcomes and is especially important for predicting **Home Wins**.
  - **HTAG** (Half-Time Away Team Goals) is more important for predicting **Away Wins**.

- **Half-Time Result (HTR)**:
  - **HTR** (Half-Time Result) plays an important role in determining match outcomes, particularly predicting **Home Wins** and **Away Wins** based on the halftime result.

- **Shots on Target**:
  - **HST** (Home Shots on Target) is significant for predicting **Home Wins**, where more shots on target decrease the likelihood of an **Away Win**.
  - **AST** (Away Shots on Target) is significant for predicting **Away Wins**, with more shots on target increasing the likelihood of an **Away Win**.

- **Red Cards**:
  - **HR** (Home Red Cards) has a significant positive effect on **Home Wins**. More home red cards increase the likelihood of a **Home Win**.
  
  - **AR** (Away Red Cards) has a minor effect on **Home Wins**, suggesting it has a slight influence on match outcomes.

- **Other Match Statistics**:
  - Most other match statistics, such as **Fouls**, **Yellow Cards**, and **Corners**, show smaller or no significant effects on match outcomes.

### Checking Multicollinearity

```{r}
vif(multinom_model)
```

The HTR (Half-Time Result) variable has a high VIF of 49.97. Also, HS and AS have a VIF of more than 16 , suggesting significant multicollinearity with other predictors in the model. So, lets check mullticollinearity by removing it.

```{r}
multinom_model.clean <- update(multinom_model, .~.-HTR -HS -AS)
vif(multinom_model.clean)

```

After removing HTR, HS, and AS from the model, the VIF results show improved multicollinearity for the remaining variables except AF and HF. So, removing AF and HF too.

```{r}
multinom_model.clean <- update(multinom_model.clean, .~.-HF -AF)
vif(multinom_model.clean)

```

Letâ€™s see the summary of the updated cleaned model.

```{r}
summary(multinom_model.clean)
```

Compare the AIC for the two models.

```{r}
AIC(multinom_model, multinom_model.clean) %>% arrange(AIC)
```

The cleaned model appears to have a higher AIC thus up to now we prefer the full initial model. Lower AIC indicates a better fit, so multinom_model is the preferred model between the two in terms of fit. multinom_model.clean has fewer predictors (22 degrees of freedom vs 34) but results in a higher AIC, suggesting it may have sacrificed model complexity without improving fit sufficiently.
