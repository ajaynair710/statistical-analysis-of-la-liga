---
title: "2_known_prep_analysis"
author: "Ajay Prakash Nair"
date: "2024-11-14"
output: html_document
---


```{r setup, include=FALSE}
# Set CRAN mirror
options(repos = c(CRAN = "https://cran.rstudio.com/"))

# Other setup configurations
knitr::opts_chunk$set(echo = TRUE)
```

# Requirements

```{r requirements, results='hide'}
requirements=c("summarytools", "pROC", "glmnetUtils", "dplyr", "car", "effects", "gridExtra", "grid", "MASS","e1071", "mgcv", "caret", "gridExtra", "effects")

for (req in requirements){
  if (!require(req, character.only = TRUE)){
      install.packages(req)
  }
}
```

# Analysis Description

This analysis focuses on building and evaluating predictive models using logistic regression and Naive Bayes for the dataset. The dataset comprises training and testing data that was splitted in the previous stage. Logistic regression and Naive Bayes is employed to predict outcomes based on a set of predictor variables. The objective is to assess the predictive performance of the models and evaluate their ability to classify outcomes accurately.

# Loading data

```{r}
train_data <- read.csv("./training/training.csv")
cat("Training data dimensions:", dim(train_data), "\n")

test_data <- read.csv("./testing/test.csv")
cat("Testing data dimensions:", dim(test_data), "\n")
```

# Logistic Regression

The logistic regression analysis involves two main stages: model building and evaluation. In the model-building stage, logistic regression models are trained using the training dataset. Two models are considered: one with all variables and another with only relevant variables. The significance of each predictor variable is assessed based on the model summary and p-values.

```{r}
# Relevel reference level as "Draw"
train_data$FTR <- relevel(factor(train_data$FTR), ref = "Draw")

library(nnet)

# Fit initial model
multinom_model <- multinom(FTR ~ HTHG + HTAG + HTR + HS + AS + HST + AST + HF + AF + HC + AC + HY + AY + HR + AR, data = train_data)

# Summary
summary(multinom_model)

```

### Key Findings:

- **Half-Time Goals**:
  - **HTHG** (Half-Time Home Team Goals) significantly influences match outcomes and is especially important for predicting **Home Wins**.
  - **HTAG** (Half-Time Away Team Goals) is more important for predicting **Away Wins**.

- **Half-Time Result (HTR)**:
  - **HTR** (Half-Time Result) plays an important role in determining match outcomes, particularly predicting **Home Wins** and **Away Wins** based on the halftime result.

- **Shots on Target**:
  - **HST** (Home Shots on Target) is significant for predicting **Home Wins**, where more shots on target decrease the likelihood of an **Away Win**.
  - **AST** (Away Shots on Target) is significant for predicting **Away Wins**, with more shots on target increasing the likelihood of an **Away Win**.

- **Red Cards**:
  - **HR** (Home Red Cards) has a significant positive effect on **Home Wins**. More home red cards increase the likelihood of a **Home Win**.
  
  - **AR** (Away Red Cards) has a minor effect on **Home Wins**, suggesting it has a slight influence on match outcomes.

- **Other Match Statistics**:
  - Most other match statistics, such as **Fouls**, **Yellow Cards**, and **Corners**, show smaller or no significant effects on match outcomes.

### Checking Multicollinearity

```{r}
vif(multinom_model)
```

The HTR (Half-Time Result) variable has a high VIF of 49.97. Also, HS and AS have a VIF of more than 16 , suggesting significant multicollinearity with other predictors in the model. So, lets check mullticollinearity by removing it.

```{r}
multinom_model.clean <- update(multinom_model, .~.-HTR -HS -AS)
vif(multinom_model.clean)

```

After removing HTR, HS, and AS from the model, the VIF results show improved multicollinearity for the remaining variables except AF and HF. So, removing AF and HF too.

```{r}
multinom_model.clean <- update(multinom_model.clean, .~.-HF -AF)
vif(multinom_model.clean)

```

Let’s see the summary of the updated cleaned model.

```{r}
summary(multinom_model.clean)
```

Compare the AIC for the two models.

```{r}
AIC(multinom_model, multinom_model.clean) %>% arrange(AIC)
```

The cleaned model appears to have a higher AIC thus up to now we prefer the full initial model. Lower AIC indicates a better fit, so multinom_model is the preferred model between the two in terms of fit. multinom_model.clean has fewer predictors (22 degrees of freedom vs 34) but results in a higher AIC, suggesting it may have sacrificed model complexity without improving fit sufficiently.

## Interaction Terms

We are adding the following interaction terms : 
HTHG × HTAG: Home half-time goals × Away half-time goals
HST × AST: Home shots on target × Away shots on target
HR × AR: Home red cards × Away red cards
HY × AY: Home yellow cards × Away yellow cards

```{r}
multinom_model.inter <- update(multinom_model, 
                         . ~ . + HTHG:HTAG + HST:AST + HR:AR + HY:AY)

summary(multinom_model.inter)

```

HST:AST, HY:AY appear to be not so much significant. So removing it.

```{r}
multinom_model.inter <- update(multinom_model.inter, . ~ . -HST:AST -HY:AY)
summary(multinom_model.inter)
```

Let's check the AIC

```{r}
AIC(multinom_model, multinom_model.clean, multinom_model.inter) %>% arrange(AIC)
```

By looking at the AIC the best model remain the initial full model. Now let’s try adding interaction terms to the clean model. Now let’s add the following interactions:

HTHG × HTAG: Home half-time goals × Away half-time goals
HST × AST: Home shots on target × Away shots on target
HR × AR: Home red cards × Away red cards
HY × AY: Home yellow cards × Away yellow cards

```{r}
multinom_model.clean.inter <- update(multinom_model.clean, 
                         . ~ . + HTHG:HTAG + HST:AST + HR:AR + HY:AY)

summary(multinom_model.clean.inter)

```

```{r}
AIC(multinom_model, multinom_model.inter, multinom_model.clean, multinom_model.clean.inter) %>% arrange(AIC)
```

And it seems like that the full model is just slightly better with respect to the full model with interaction terms.

## Model Interpretation

We have decided to interpret the model multinom_model based on its lower AIC value (5860.762), which suggests that it provides the best balance between fit and complexity.

To begin the interpretation, we recall the model’s summary:

```{r}
summary(multinom_model)
```

```{r}
# Plot the effects for 4 interactionless variables from the multinom_model

# Plot for HTHG (Home Team Home Goals)
a <- effect("HTHG", multinom_model)
plot(a, rescale.axis = FALSE, ylab = "Probability of FTR")

# Plot for HTAG (Home Team Away Goals)
b <- effect("HTAG", multinom_model)
plot(b, rescale.axis = FALSE, ylab = "Probability of FTR")

# Plot for HST (Home Team Shots)
c <- effect("HST", multinom_model)
plot(c, rescale.axis = FALSE, ylab = "Probability of FTR")

# Plot for AST (Away Team Shots)
d <- effect("AST", multinom_model)
plot(d, rescale.axis = FALSE, ylab = "Probability of FTR")

```

As we can see, if Half Time Home Team Goals (HTHG) are greater than 3, the probability of the home team winning is 1; if Half Time Away Goals (HTAG) are greater than 2, the probability of the away team winning is more than 50%; if the Home Team Shots on Target (HST) are greater than 10, the probability of the home team winning is high; and if the Away Team Shots on Target (AST) are greater than 10, the probability of the away team winning is high.

## Model Comparison

Let’s compare all the models done so far in terms of prediction power to be able to compare them with other types of models: