---
title: "2_known_prep_analysis"
author: "Ajay Prakash Nair"
date: "2024-11-14"
output: html_document
---


```{r setup, include=FALSE}
# Set CRAN mirror
options(repos = c(CRAN = "https://cran.rstudio.com/"))

# Other setup configurations
knitr::opts_chunk$set(echo = TRUE)
```

# Requirements

```{r requirements, results='hide'}
requirements=c("summarytools", "pROC", "glmnetUtils", "dplyr", "car", "effects", "gridExtra", "grid", "MASS","e1071", "mgcv", "caret", "gridExtra", "effects", "nnet")

for (req in requirements){
  if (!require(req, character.only = TRUE)){
      install.packages(req)
  }
}
```

# Analysis Description

This analysis focuses on building and evaluating predictive models using logistic regression and Naive Bayes for the dataset. The dataset comprises training and testing data that was splitted in the previous stage. Logistic regression and Naive Bayes is employed to predict outcomes based on a set of predictor variables. The objective is to assess the predictive performance of the models and evaluate their ability to classify outcomes accurately.

# Loading data

```{r}
train_data <- read.csv("./training/training.csv")
cat("Training data dimensions:", dim(train_data), "\n")

test_data <- read.csv("./testing/test.csv")
cat("Testing data dimensions:", dim(test_data), "\n")
```

# Logistic Regression

The logistic regression analysis involves two main stages: model building and evaluation. In the model-building stage, logistic regression models are trained using the training dataset. Two models are considered: one with all variables and another with only relevant variables. The significance of each predictor variable is assessed based on the model summary and p-values.

```{r}
# Relevel reference level as "Draw"
train_data$FTR <- relevel(factor(train_data$FTR), ref = "Draw")

library(nnet)

# Fit initial model
multinom_model <- multinom(FTR ~ HTHG + HTAG + HTR + HS + AS + HST + AST + HF + AF + HC + AC + HY + AY + HR + AR, data = train_data)

# Summary
summary(multinom_model)

```

### Key Findings:

- **Half-Time Goals**:
  - **HTHG** (Half-Time Home Team Goals) significantly influences match outcomes and is especially important for predicting **Home Wins**.
  - **HTAG** (Half-Time Away Team Goals) is more important for predicting **Away Wins**.

- **Half-Time Result (HTR)**:
  - **HTR** (Half-Time Result) plays an important role in determining match outcomes, particularly predicting **Home Wins** and **Away Wins** based on the halftime result.

- **Shots on Target**:
  - **HST** (Home Shots on Target) is significant for predicting **Home Wins**, where more shots on target decrease the likelihood of an **Away Win**.
  - **AST** (Away Shots on Target) is significant for predicting **Away Wins**, with more shots on target increasing the likelihood of an **Away Win**.

- **Red Cards**:
  - **HR** (Home Red Cards) has a significant positive effect on **Home Wins**. More home red cards increase the likelihood of a **Home Win**.
  
  - **AR** (Away Red Cards) has a minor effect on **Home Wins**, suggesting it has a slight influence on match outcomes.

- **Other Match Statistics**:
  - Most other match statistics, such as **Fouls**, **Yellow Cards**, and **Corners**, show smaller or no significant effects on match outcomes.

### Checking Multicollinearity

```{r}
vif(multinom_model)
```

The HTR (Half-Time Result) variable has a high VIF of 49.97. Also, HS and AS have a VIF of more than 16 , suggesting significant multicollinearity with other predictors in the model. So, lets check mullticollinearity by removing it.

```{r}
multinom_model.clean <- update(multinom_model, .~.-HTR -HS -AS)
vif(multinom_model.clean)

```

After removing HTR, HS, and AS from the model, the VIF results show improved multicollinearity for the remaining variables except AF and HF. So, removing AF and HF too.

```{r}
multinom_model.clean <- update(multinom_model.clean, .~.-HF -AF)
vif(multinom_model.clean)

```

Let’s see the summary of the updated cleaned model.

```{r}
summary(multinom_model.clean)
```

Compare the AIC for the two models.

```{r}
AIC(multinom_model, multinom_model.clean) %>% arrange(AIC)
```

The cleaned model appears to have a higher AIC thus up to now we prefer the full initial model. Lower AIC indicates a better fit, so multinom_model is the preferred model between the two in terms of fit. multinom_model.clean has fewer predictors (22 degrees of freedom vs 34) but results in a higher AIC, suggesting it may have sacrificed model complexity without improving fit sufficiently.

## Interaction Terms

We are adding the following interaction terms : 
HTHG × HTAG: Home half-time goals × Away half-time goals
HST × AST: Home shots on target × Away shots on target
HR × AR: Home red cards × Away red cards
HY × AY: Home yellow cards × Away yellow cards

```{r}
multinom_model.inter <- update(multinom_model, 
                         . ~ . + HTHG:HTAG + HST:AST + HR:AR + HY:AY)

summary(multinom_model.inter)

```

HST:AST, HY:AY appear to be not so much significant. So removing it.

```{r}
multinom_model.inter <- update(multinom_model.inter, . ~ . -HST:AST -HY:AY)
summary(multinom_model.inter)
```

Let's check the AIC

```{r}
AIC(multinom_model, multinom_model.clean, multinom_model.inter) %>% arrange(AIC)
```

By looking at the AIC the best model remain the initial full model. Now let’s try adding interaction terms to the clean model. Now let’s add the following interactions:

HTHG × HTAG: Home half-time goals × Away half-time goals
HST × AST: Home shots on target × Away shots on target
HR × AR: Home red cards × Away red cards
HY × AY: Home yellow cards × Away yellow cards

```{r}
multinom_model.clean.inter <- update(multinom_model.clean, 
                         . ~ . + HTHG:HTAG + HST:AST + HR:AR + HY:AY)

summary(multinom_model.clean.inter)

```

```{r}
AIC(multinom_model, multinom_model.inter, multinom_model.clean, multinom_model.clean.inter) %>% arrange(AIC)
```

And it seems like that the full model is just slightly better with respect to the full model with interaction terms.

## Model Interpretation

We have decided to interpret the model multinom_model based on its lower AIC value (5860.762), which suggests that it provides the best balance between fit and complexity.

To begin the interpretation, we recall the model’s summary:

```{r}
summary(multinom_model)
```

```{r}
# Plot the effects for 4 interactionless variables from the multinom_model

# Plot for HTHG (Home Team Home Goals)
a <- effect("HTHG", multinom_model)
plot(a, rescale.axis = FALSE, ylab = "Probability of FTR")

# Plot for HTAG (Home Team Away Goals)
b <- effect("HTAG", multinom_model)
plot(b, rescale.axis = FALSE, ylab = "Probability of FTR")

# Plot for HST (Home Team Shots)
c <- effect("HST", multinom_model)
plot(c, rescale.axis = FALSE, ylab = "Probability of FTR")

# Plot for AST (Away Team Shots)
d <- effect("AST", multinom_model)
plot(d, rescale.axis = FALSE, ylab = "Probability of FTR")

```

As we can see, if Half Time Home Team Goals (HTHG) are greater than 3, the probability of the home team winning is 1; if Half Time Away Goals (HTAG) are greater than 2, the probability of the away team winning is more than 50%; if the Home Team Shots on Target (HST) are greater than 10, the probability of the home team winning is high; and if the Away Team Shots on Target (AST) are greater than 10, the probability of the away team winning is high.

## Model Comparison

Let’s compare all the models done so far in terms of prediction power to be able to compare them with other types of models:

```{r}
# Load the test dataset
test_data <- read.csv("testing/test.csv")
```

```{r}
# Load required libraries
library(pROC)
library(caret)

# Compute probabilities for each model
prob_multinom_model <- predict(multinom_model, test_data, type = "prob")
prob_multinom_model.inter <- predict(multinom_model.inter, test_data, type = "prob")
prob_multinom_model.clean <- predict(multinom_model.clean, test_data, type = "prob")
prob_multinom_model.clean.inter <- predict(multinom_model.clean.inter, test_data, type = "prob")

# Set up the 2x2 plotting area
par(mfrow = c(2, 2))

# Plot ROC curve for each model with AUC
roc_multinom_model <- roc(test_data$FTR ~ prob_multinom_model[,1], plot = TRUE, 
                          print.auc = TRUE, main = "multinom_model ROC Curve")

roc_multinom_model.inter <- roc(test_data$FTR ~ prob_multinom_model.inter[,1], plot = TRUE, 
                                 print.auc = TRUE, main = "multinom_model.inter ROC Curve")

roc_multinom_model.clean <- roc(test_data$FTR ~ prob_multinom_model.clean[,1], plot = TRUE, 
                                 print.auc = TRUE, main = "multinom_model.clean ROC Curve")

roc_multinom_model.clean.inter <- roc(test_data$FTR ~ prob_multinom_model.clean.inter[,1], plot = TRUE, 
                                       print.auc = TRUE, main = "multinom_model.clean.inter ROC Curve")

```

By the plots we can see that the Area Under the Curve (AUC) computed by the ROC curves gives almost the same results.

Now we extract from each roc object the best threshold in order to use it during predictions.

```{r}
# Extract best thresholds and metrics for each ROC object
roc_metrics_full <- coords(roc_multinom_model, x = "best", ret = "all")
roc_metrics_full_inter <- coords(roc_multinom_model.inter, x = "best", ret = "all")
roc_metrics_clean <- coords(roc_multinom_model.clean, x = "best", ret = "all")
roc_metrics_clean_inter <- coords(roc_multinom_model.clean.inter, x = "best", ret = "all")
```

Rename the single row for each dataframe and concatenate all in a single one.

```{r}
# Rename rows to identify models
row.names(roc_metrics_full) <- "Full Model"
row.names(roc_metrics_full_inter) <- "Interaction Full Model"
row.names(roc_metrics_clean) <- "Cleaned Model"
row.names(roc_metrics_clean_inter) <- "Interaction Cleaned Model"

# Combine all metrics into a single dataframe
roc_results <- rbind(
  roc_metrics_full,
  roc_metrics_full_inter,
  roc_metrics_clean,
  roc_metrics_clean_inter
)
```

Now we make a comparisons result: 

1. Specificity

```{r}
library(dplyr)
roc_results_df <- as.data.frame(roc_results)

# Select and arrange specificity
specificity_comparison <- roc_results_df %>%
  dplyr::select(specificity) %>%
  dplyr::arrange(desc(specificity))

# View the results
print(specificity_comparison)
```

In terms of specificity the best model is the Interaction Full Model.

2. Sensitivity
```{r}
sensitivity_comparison <- roc_results_df %>%
  dplyr::select(sensitivity) %>%
  dplyr::arrange(desc(sensitivity))

print(sensitivity_comparison)
```

Regarding the sensitivity the best model is the Cleaned Model.

3. Accuracy 

```{r}
accuracy_comparison <- roc_results_df %>%
  dplyr::select(accuracy) %>%
  dplyr::arrange(desc(accuracy))

print(accuracy_comparison)
```

And finally for the accuracy measure the best model is the Interaction Full Model.

Also, we can see that the response is unbalanced.

### Confusion Matrixes

Let’s have a quick look on the confusion matrix for the four model that we have specified.

```{r}
# Extract the thresholds from the ROC metrics
threshold_full <- roc_metrics_full$threshold
threshold_full_inter <- roc_metrics_full_inter$threshold
threshold_clean <- roc_metrics_clean$threshold
threshold_clean_inter <- roc_metrics_clean_inter$threshold

# For multinomial classification, select the class with the highest probability
pred_full <- apply(prob_multinom_model, 1, function(x) which.max(x))
pred_full_inter <- apply(prob_multinom_model.inter, 1, function(x) which.max(x))
pred_clean <- apply(prob_multinom_model.clean, 1, function(x) which.max(x))
pred_clean_inter <- apply(prob_multinom_model.clean.inter, 1, function(x) which.max(x))
```

1. Full Model 

```{r}
# Create confusion matrices
conf_matrix_full <- table(pred = pred_full, true = as.factor(test_data$FTR))
cat("Full Model Confusion Matrix:\n")
print(conf_matrix_full)
```

2. Interaction Full Model

```{r}
conf_matrix_full_inter <- table(pred = pred_full_inter, true = as.factor(test_data$FTR))
cat("Interaction Full Model Confusion Matrix:\n")
print(conf_matrix_full_inter)
```

3. Cleaned Model

```{r}
conf_matrix_clean <- table(pred = pred_clean, true = as.factor(test_data$FTR))
cat("Cleaned Model Confusion Matrix:\n")
print(conf_matrix_clean)
```

4. Interaction Cleaned Model

```{r}
conf_matrix_clean_inter <- table(pred = pred_clean_inter, true = as.factor(test_data$FTR))
cat("Interaction Cleaned Model Confusion Matrix:\n")
print(conf_matrix_clean_inter)
```

## Model Selection

Now let’s apply automatic model selection to identity the best model.

```{r}
# Load libraries
install.packages("MASS")
install.packages("nnet")
library(MASS)
library(nnet)

# Fit a full multinomial model
full_model <- multinom(FTR ~ HTHG + HTAG + HTR + HS + AS + HST + AST + HF + AF + HC + AC + HY + AY + HR + AR, data = test_data)

# Apply stepwise selection
stepwise_model <- stepAIC(full_model, direction = "both", trace = 1)

# View the summary of the stepwise selected model
summary(stepwise_model)
```

```{r}
# Calculate AIC for each model
aic_full <- AIC(multinom_model)
aic_full_inter <- AIC(multinom_model.inter)
aic_clean <- AIC(multinom_model.clean)
aic_clean_inter <- AIC(multinom_model.clean.inter)
aic_stepwise <- AIC(stepwise_model)

# Create a data frame with the models and their AIC values
aic_results <- data.frame(
  Model = c("Full Model", "Interaction Full Model", "Cleaned Model", "Interaction Cleaned Model", "Stepwise Model"),
  AIC = c(aic_full, aic_full_inter, aic_clean, aic_clean_inter, aic_stepwise)
)

# Arrange the models by AIC (ascending order)
aic_results_sorted <- aic_results %>% arrange(AIC)

# Print the sorted AIC results
print(aic_results_sorted)
```

In terms of AIC the “Step model” is better.

```{r}
# Get the predicted probabilities for the stepwise model
prob_stepwise <- predict(stepwise_model, test_data, type = "prob")

# Define true labels for the ROC curve
true_labels <- test_data$FTR  # Adjust if needed

# Plot the ROC curve for the stepwise model
roc_stepwise <- roc(true_labels, prob_stepwise[,1], plot = TRUE, print.auc = TRUE, main = "Stepwise Model ROC Curve")
```

```{r}
# Convert probabilities into class predictions by selecting the class with the highest probability
pred_stepwise_multiclass <- apply(prob_stepwise, 1, function(x) which.max(x)) 

```

```{r}
# Create confusion matrix for the Stepwise Model
conf_matrix_stepwise <- table(pred = pred_stepwise_multiclass, true = as.factor(test_data$FTR))

# Print the confusion matrix
cat("Stepwise Model Confusion Matrix:\n")
print(conf_matrix_stepwise)

```

### Lasso Regression

```{r}
# Convert predictors to a matrix
x <- model.matrix(FTR ~ ., data = train_data)[, -1]  # Exclude the intercept

# Response variable
y <- train_data$FTR  # Ensure this is a factor for multinomial classification

# Fit LASSO regression model
model_fit.lasso <- glmnet(
  x, 
  y, 
  family = "multinomial",  # Corrected the argument
  alpha = 1  # LASSO regularization
)

# Plot the LASSO model
plot(model_fit.lasso, xvar = "lambda", label = TRUE)

```

We choose lambda using cross validation:

```{r}
modelcv.lasso <- cv.glmnet(
  x, 
  y, 
  alpha = 1,  # LASSO penalty
  family = "multinomial"  # Multinomial classification
)

# Plot cross-validation results
plot(modelcv.lasso)
```

The value of λ that minimizes the ridge cross-validated mean square error is:

```{r}
modelcv.lasso$lambda.min
```

Again like before we take as λ value corresponding to one standard error from the minimum of the cross-validated mean square error:

```{r}
model_bestlam.lasso <- modelcv.lasso$lambda.1se
model_bestlam.lasso
```

Like before, now visualize again the lasso estimates as a function of the logarithm of λ and add a vertical line corresponding to best.lambda:

```{r}
plot(model_fit.lasso)
abline(v = log(model_bestlam.lasso), lwd = 1.2, lty = "dashed")
```

These are the coefficients:

```{r}
# Extract coefficients at the best lambda
model_bestlam.lasso.coef <- coef(model_fit.lasso, s = model_bestlam.lasso)

# Handle potential list or sparse matrix cases
if (is.list(model_bestlam.lasso.coef)) {
  coef_matrix <- as.matrix(model_bestlam.lasso.coef[[1]])
} else {
  coef_matrix <- as.matrix(model_bestlam.lasso.coef)
}

# Filter non-zero coefficients
nonzero_coefs <- coef_matrix[coef_matrix != 0, , drop = FALSE]
print(nonzero_coefs)

```
