---
title: "2_known_prep_analysis"
author: "Ajay Prakash Nair"
date: "2024-11-14"
output: html_document
---


```{r setup, include=FALSE}
# Set CRAN mirror
options(repos = c(CRAN = "https://cran.rstudio.com/"))

# Other setup configurations
knitr::opts_chunk$set(echo = TRUE)
```

# Requirements

```{r requirements, results='hide'}
requirements=c("summarytools", "pROC", "glmnetUtils", "dplyr", "car", "effects", "gridExtra", "grid", "MASS","e1071", "mgcv", "caret", "gridExtra", "effects", "nnet", "ggplot2", "reshape2")

for (req in requirements){
  if (!require(req, character.only = TRUE)){
      install.packages(req)
  }
}
```

# Analysis Description

This analysis focuses on building and evaluating predictive models using logistic regression and Naive Bayes for the dataset. The dataset comprises training and testing data that was splitted in the previous stage. Logistic regression and Naive Bayes is employed to predict outcomes based on a set of predictor variables. The objective is to assess the predictive performance of the models and evaluate their ability to classify outcomes accurately.

# Loading data

```{r}
train_data <- read.csv("./training/training.csv")
cat("Training data dimensions:", dim(train_data), "\n")

test_data <- read.csv("./testing/test.csv")
cat("Testing data dimensions:", dim(test_data), "\n")
```

# Logistic Regression

The logistic regression analysis involves two main stages: model building and evaluation. In the model-building stage, logistic regression models are trained using the training dataset. Two models are considered: one with all variables and another with only relevant variables. The significance of each predictor variable is assessed based on the model summary and p-values.

```{r}
# Relevel reference level as "Draw"
train_data$FTR <- relevel(factor(train_data$FTR), ref = "Draw")

library(nnet)

# Fit initial model
multinom_model <- multinom(FTR ~ HTHG + HTAG + HTR + HS + AS + HST + AST + HF + AF + HC + AC + HY + AY + HR + AR, data = train_data)

# Summary
summary(multinom_model)

```

### Key Findings:

- **Half-Time Goals**:
  - **HTHG** (Half-Time Home Team Goals) significantly influences match outcomes and is especially important for predicting **Home Wins**.
  - **HTAG** (Half-Time Away Team Goals) is more important for predicting **Away Wins**.

- **Half-Time Result (HTR)**:
  - **HTR** (Half-Time Result) plays an important role in determining match outcomes, particularly predicting **Home Wins** and **Away Wins** based on the halftime result.

- **Shots on Target**:
  - **HST** (Home Shots on Target) is significant for predicting **Home Wins**, where more shots on target decrease the likelihood of an **Away Win**.
  - **AST** (Away Shots on Target) is significant for predicting **Away Wins**, with more shots on target increasing the likelihood of an **Away Win**.

- **Red Cards**:
  - **HR** (Home Red Cards) has a significant positive effect on **Home Wins**. More home red cards increase the likelihood of a **Home Win**.
  
  - **AR** (Away Red Cards) has a minor effect on **Home Wins**, suggesting it has a slight influence on match outcomes.

- **Other Match Statistics**:
  - Most other match statistics, such as **Fouls**, **Yellow Cards**, and **Corners**, show smaller or no significant effects on match outcomes.

### Checking Multicollinearity

```{r}
vif(multinom_model)
```

The HTR (Half-Time Result) variable has a high VIF of 49.97. Also, HS and AS have a VIF of more than 16 , suggesting significant multicollinearity with other predictors in the model. So, lets check mullticollinearity by removing it.

```{r}
multinom_model.clean <- update(multinom_model, .~.-HTR -HS -AS)
vif(multinom_model.clean)

```

After removing HTR, HS, and AS from the model, the VIF results show improved multicollinearity for the remaining variables except AF and HF. So, removing AF and HF too.

```{r}
multinom_model.clean <- update(multinom_model.clean, .~.-HF -AF)
vif(multinom_model.clean)

```

Let’s see the summary of the updated cleaned model.

```{r}
summary(multinom_model.clean)
```

Compare the AIC for the two models.

```{r}
AIC(multinom_model, multinom_model.clean) %>% arrange(AIC)
```

The cleaned model appears to have a higher AIC thus up to now we prefer the full initial model. Lower AIC indicates a better fit, so multinom_model is the preferred model between the two in terms of fit. multinom_model.clean has fewer predictors (22 degrees of freedom vs 34) but results in a higher AIC, suggesting it may have sacrificed model complexity without improving fit sufficiently.

## Interaction Terms

We are adding the following interaction terms : 
HTHG × HTAG: Home half-time goals × Away half-time goals
HST × AST: Home shots on target × Away shots on target
HR × AR: Home red cards × Away red cards
HY × AY: Home yellow cards × Away yellow cards

```{r}
multinom_model.inter <- update(multinom_model, 
                         . ~ . + HTHG:HTAG + HST:AST + HR:AR + HY:AY)

summary(multinom_model.inter)

```

HST:AST, HY:AY appear to be not so much significant. So removing it.

```{r}
multinom_model.inter <- update(multinom_model.inter, . ~ . -HST:AST -HY:AY)
summary(multinom_model.inter)
```

Let's check the AIC

```{r}
AIC(multinom_model, multinom_model.clean, multinom_model.inter) %>% arrange(AIC)
```

By looking at the AIC the best model remain the initial full model. Now let’s try adding interaction terms to the clean model. Now let’s add the following interactions:

HTHG × HTAG: Home half-time goals × Away half-time goals
HST × AST: Home shots on target × Away shots on target
HR × AR: Home red cards × Away red cards
HY × AY: Home yellow cards × Away yellow cards

```{r}
multinom_model.clean.inter <- update(multinom_model.clean, 
                         . ~ . + HTHG:HTAG + HST:AST + HR:AR + HY:AY)

summary(multinom_model.clean.inter)

```

```{r}
AIC(multinom_model, multinom_model.inter, multinom_model.clean, multinom_model.clean.inter) %>% arrange(AIC)
```

And it seems like that the full model is just slightly better with respect to the full model with interaction terms.

## Model Interpretation

We have decided to interpret the model multinom_model based on its lower AIC value (5860.762), which suggests that it provides the best balance between fit and complexity.

To begin the interpretation, we recall the model’s summary:

```{r}
summary(multinom_model)
```

```{r}
# Plot the effects for 4 interactionless variables from the multinom_model

# Plot for HTHG (Home Team Home Goals)
a <- effect("HTHG", multinom_model)
plot(a, rescale.axis = FALSE, ylab = "Probability of FTR")

# Plot for HTAG (Home Team Away Goals)
b <- effect("HTAG", multinom_model)
plot(b, rescale.axis = FALSE, ylab = "Probability of FTR")

# Plot for HST (Home Team Shots)
c <- effect("HST", multinom_model)
plot(c, rescale.axis = FALSE, ylab = "Probability of FTR")

# Plot for AST (Away Team Shots)
d <- effect("AST", multinom_model)
plot(d, rescale.axis = FALSE, ylab = "Probability of FTR")

```

As we can see, if Half Time Home Team Goals (HTHG) are greater than 3, the probability of the home team winning is 1; if Half Time Away Goals (HTAG) are greater than 2, the probability of the away team winning is more than 50%; if the Home Team Shots on Target (HST) are greater than 10, the probability of the home team winning is high; and if the Away Team Shots on Target (AST) are greater than 10, the probability of the away team winning is high.

## Model Comparison

Let’s compare all the models done so far in terms of prediction power to be able to compare them with other types of models:

```{r}
# Load the test dataset
test_data <- read.csv("testing/test.csv")
```

```{r}
# Function to plot ROC curves for a model
plot_roc <- function(test_data, prob_matrix, model_name) {
  roc_list <- lapply(1:ncol(prob_matrix), function(i) {
    roc(test_data$FTR == colnames(prob_matrix)[i], prob_matrix[, i])
  })
  
  # Extract AUC values
  auc_values <- sapply(roc_list, function(r) auc(r))
  class_names <- colnames(prob_matrix)
  
  # Prepare data for ggplot
  roc_data <- do.call(rbind, lapply(1:length(roc_list), function(i) {
    data.frame(
      Specificity = 1 - roc_list[[i]]$specificities,
      Sensitivity = roc_list[[i]]$sensitivities,
      Class = class_names[i]
    )
  }))
  
  # Plot using ggplot
  ggplot(data = roc_data, aes(x = Specificity, y = Sensitivity, color = Class)) +
    geom_line(size = 1) +
    labs(
      title = paste("ROC Curve for", model_name),
      subtitle = paste("AUC:", 
                       paste(class_names, round(auc_values, 3), collapse = ", ")),
      x = "1 - Specificity",
      y = "Sensitivity"
    ) +
    theme_minimal() +
    theme(plot.title = element_text(size = 14, face = "bold"),
          plot.subtitle = element_text(size = 12),
          legend.title = element_text(size = 12),
          legend.text = element_text(size = 10)) +
    scale_color_brewer(palette = "Set1")
}

# Compute probabilities for each model
prob_multinom_model <- predict(multinom_model, test_data, type = "prob")
prob_multinom_model.inter <- predict(multinom_model.inter, test_data, type = "prob")
prob_multinom_model.clean <- predict(multinom_model.clean, test_data, type = "prob")
prob_multinom_model.clean.inter <- predict(multinom_model.clean.inter, test_data, type = "prob")

# Plot ROC curves for each model
plot_roc(test_data, prob_multinom_model, "Multinom Model")
plot_roc(test_data, prob_multinom_model.inter, "Multinom Model with Interaction")
plot_roc(test_data, prob_multinom_model.clean, "Cleaned Multinom Model")
plot_roc(test_data, prob_multinom_model.clean.inter, "Cleaned Multinom Model with Interaction")

```

By the plots we can see that the Area Under the Curve (AUC) computed by the ROC curves gives almost the same results.

Now we extract from each roc object the best threshold in order to use it during predictions.

```{r}
# Compute individual ROC curves for each class
roc_list_full <- lapply(1:ncol(prob_multinom_model), function(i) {
  roc(test_data$FTR == colnames(prob_multinom_model)[i], prob_multinom_model[, i])
})
roc_list_full_inter <- lapply(1:ncol(prob_multinom_model.inter), function(i) {
  roc(test_data$FTR == colnames(prob_multinom_model.inter)[i], prob_multinom_model.inter[, i])
})
roc_list_clean <- lapply(1:ncol(prob_multinom_model.clean), function(i) {
  roc(test_data$FTR == colnames(prob_multinom_model.clean)[i], prob_multinom_model.clean[, i])
})
roc_list_clean_inter <- lapply(1:ncol(prob_multinom_model.clean.inter), function(i) {
  roc(test_data$FTR == colnames(prob_multinom_model.clean.inter)[i], prob_multinom_model.clean.inter[, i])
})

# Extract best thresholds and metrics for each class
extract_metrics <- function(roc_list) {
  lapply(roc_list, function(r) coords(r, x = "best", ret = c("threshold", "specificity", "sensitivity", "accuracy")))
}

roc_metrics_full <- extract_metrics(roc_list_full)
roc_metrics_full_inter <- extract_metrics(roc_list_full_inter)
roc_metrics_clean <- extract_metrics(roc_list_clean)
roc_metrics_clean_inter <- extract_metrics(roc_list_clean_inter)
```

Rename the single row for each dataframe and concatenate all in a single one.

```{r}
# Combine metrics for each model
combine_metrics <- function(roc_metrics_list) {
  metrics <- do.call(rbind, roc_metrics_list)
  rownames(metrics) <- colnames(prob_multinom_model)
  metrics
}

roc_metrics_full <- combine_metrics(roc_metrics_full)
roc_metrics_full_inter <- combine_metrics(roc_metrics_full_inter)
roc_metrics_clean <- combine_metrics(roc_metrics_clean)
roc_metrics_clean_inter <- combine_metrics(roc_metrics_clean_inter)

# Combine all metrics into a single dataframe
roc_results <- rbind(
  roc_metrics_full,
  roc_metrics_full_inter,
  roc_metrics_clean,
  roc_metrics_clean_inter
)
```

Now we make a comparisons result: 

```{r}
# Compute average metrics across classes for each model
average_metrics <- function(metrics) {
  colMeans(metrics, na.rm = TRUE)
}

average_metrics_full <- average_metrics(roc_metrics_full)
average_metrics_full_inter <- average_metrics(roc_metrics_full_inter)
average_metrics_clean <- average_metrics(roc_metrics_clean)
average_metrics_clean_inter <- average_metrics(roc_metrics_clean_inter)

# Combine all model metrics into a single dataframe
comparison_df <- data.frame(
  Model = c("Full Model", "Full Interaction Model", "Cleaned Model", "Cleaned Interaction Model"),
  Accuracy = c(average_metrics_full["accuracy"], 
               average_metrics_full_inter["accuracy"],
               average_metrics_clean["accuracy"], 
               average_metrics_clean_inter["accuracy"]),
  Sensitivity = c(average_metrics_full["sensitivity"], 
                  average_metrics_full_inter["sensitivity"],
                  average_metrics_clean["sensitivity"], 
                  average_metrics_clean_inter["sensitivity"]),
  Specificity = c(average_metrics_full["specificity"], 
                  average_metrics_full_inter["specificity"],
                  average_metrics_clean["specificity"], 
                  average_metrics_clean_inter["specificity"])
)

# Print the comparison table
print(comparison_df)
```

The Full Model is best for accuracy, the Full Interaction Model excels in sensitivity, and the Cleaned Model performs best in specificity. We can see that the response is unbalanced.

### Confusion Matrixes

Let’s have a quick look on the confusion matrix for the four model that we have specified.

```{r}
test_data$FTR <- factor(test_data$FTR, levels = c("Away Win", "Draw", "Home Win"))

# Predict class labels for each model using the best thresholds
pred_multinom_model <- factor(predict(multinom_model, test_data, type = "class"), levels = c("Away Win", "Draw", "Home Win"))
pred_multinom_model.inter <- factor(predict(multinom_model.inter, test_data, type = "class"), levels = c("Away Win", "Draw", "Home Win"))
pred_multinom_model.clean <- factor(predict(multinom_model.clean, test_data, type = "class"), levels = c("Away Win", "Draw", "Home Win"))
pred_multinom_model.clean.inter <- factor(predict(multinom_model.clean.inter, test_data, type = "class"), levels = c("Away Win", "Draw", "Home Win"))

# Generate confusion matrices for each model
conf_matrix_multinom_model <- confusionMatrix(pred_multinom_model, test_data$FTR)
conf_matrix_multinom_model.inter <- confusionMatrix(pred_multinom_model.inter, test_data$FTR)
conf_matrix_multinom_model.clean <- confusionMatrix(pred_multinom_model.clean, test_data$FTR)
conf_matrix_multinom_model.clean.inter <- confusionMatrix(pred_multinom_model.clean.inter, test_data$FTR)
```

1. Full Model


```{r}
print(conf_matrix_multinom_model)
```

2. Full Interaction Model

```{r}
print(conf_matrix_multinom_model.inter)
```

3. Cleaned Model


```{r}
print(conf_matrix_multinom_model.clean)
```

4. Cleaned Interaction Model


```{r}
print(conf_matrix_multinom_model.clean.inter)
```

## Model Selection

### Stepwise Model

Now let’s apply automatic model selection to identity the best model.

```{r}
# Fit a full multinomial model
full_model <- multinom(FTR ~ HTHG + HTAG + HTR + HS + AS + HST + AST + HF + AF + HC + AC + HY + AY + HR + AR, data = test_data)

# Apply stepwise selection
stepwise_model <- stepAIC(full_model, direction = "both", trace = 1)

# View the summary of the stepwise selected model
summary(stepwise_model)
```

```{r}
# Calculate AIC for each model
aic_full <- AIC(multinom_model)
aic_full_inter <- AIC(multinom_model.inter)
aic_clean <- AIC(multinom_model.clean)
aic_clean_inter <- AIC(multinom_model.clean.inter)
aic_stepwise <- AIC(stepwise_model)

# Create a data frame with the models and their AIC values
aic_results <- data.frame(
  Model = c("Full Model", "Interaction Full Model", "Cleaned Model", "Interaction Cleaned Model", "Stepwise Model"),
  AIC = c(aic_full, aic_full_inter, aic_clean, aic_clean_inter, aic_stepwise)
)

# Arrange the models by AIC (ascending order)
aic_results_sorted <- aic_results %>% arrange(AIC)

# Print the sorted AIC results
print(aic_results_sorted)
```

In terms of AIC the “Step model” is better.

```{r}
# Compute probabilities for the stepwise model
prob_stepwise_model <- predict(stepwise_model, test_data, type = "prob")

# Plot ROC curve for stepwise model
plot_roc(test_data, prob_stepwise_model, "Stepwise Model")
```

Confusion Matrix for stepwise model

```{r}
# Make sure that the response and prediction are factors with the same levels
pred_stepwise <- predict(stepwise_model, test_data, type = "class")
pred_stepwise <- factor(pred_stepwise, levels = levels(test_data$FTR))

# Confusion matrix for the stepwise model
conf_matrix_stepwise <- confusionMatrix(pred_stepwise, test_data$FTR)

# Print the confusion matrix for the stepwise model
print(conf_matrix_stepwise)
```

Let's compare it with other models

```{r}
# Extract metrics for each class
specificity_awaywin <- conf_matrix_stepwise$byClass["Class: Away Win", "Specificity"]
sensitivity_awaywin <- conf_matrix_stepwise$byClass["Class: Away Win", "Sensitivity"]

specificity_draw <- conf_matrix_stepwise$byClass["Class: Draw", "Specificity"]
sensitivity_draw <- conf_matrix_stepwise$byClass["Class: Draw", "Sensitivity"]

specificity_homewin <- conf_matrix_stepwise$byClass["Class: Home Win", "Specificity"]
sensitivity_homewin <- conf_matrix_stepwise$byClass["Class: Home Win", "Sensitivity"]

accuracy_stepwise <- conf_matrix_stepwise$overall["Accuracy"]

# Overall Sensitivity
overall_sensitivity <- mean(c(sensitivity_awaywin, sensitivity_draw, sensitivity_homewin))

# Overall Specificity
overall_specificity <- mean(c(specificity_awaywin, specificity_draw, specificity_homewin))

# Overall Accuracy
overall_accuracy <- accuracy_stepwise

# Combine these metrics into a single data frame
metrics_stepwise_overall <- data.frame(
  Model = "Stepwise Model",
  Accuracy = overall_accuracy,
  Sensitivity = overall_sensitivity,  
  Specificity = overall_specificity 
)

comparison_df <- rbind(
  comparison_df,  
  metrics_stepwise_overall
)

# Print the comparison
print(comparison_df)

```

### Lasso Regression

```{r}
# Prepare the data
x <- model.matrix(FTR ~ ., data = train_data)[, -1]
y <- train_data$FTR  

# Scale predictors
x_scaled <- scale(x)

# Fit LASSO regression model
model_fit.lasso <- glmnet(x_scaled, y, family = "multinomial", alpha = 1)

# Plot the LASSO model
plot(model_fit.lasso, xvar = "lambda", label = TRUE)
```

We choose lambda using cross validation:

```{r}
# Fit Lasso model using cross-validation to choose lambda
modelcv.lasso <- cv.glmnet(x_scaled, y, alpha = 1, family = "multinomial")

# Plot the lasso path
plot(modelcv.lasso)
```

The value of λ that minimizes the ridge cross-validated mean square error is:

```{r}
lambda_min <- modelcv.lasso$lambda.min
lambda_min
```

We take as λ value corresponding to one standard error from the minimum of the cross-validated mean square error:


```{r}
lambda_1se <- modelcv.lasso$lambda.1se
lambda_1se
```

Now visualize again the lasso estimates as a function of the logarithm of λ and add a vertical line corresponding to best.lambda:

```{r}
plot(model_fit.lasso, xvar = "lambda", label = TRUE)
abline(v = log(lambda_1se), lwd = 1.2, lty = "dashed")
```

These are the coefficients:

```{r}
# Extract coefficients at the best lambda
model_bestlam.lasso.coef <- coef(model_fit.lasso, s = lambda_1se)

# Process coefficients for each class
nonzero_coefs <- lapply(model_bestlam.lasso.coef, function(coef_matrix) {
  coef_matrix <- as.matrix(coef_matrix)
  coef_matrix[coef_matrix != 0, , drop = FALSE]
})

print(nonzero_coefs)
```

Now we apply prediction on the test set.

```{r}

# Prepare the data for the training set
x_train <- model.matrix(FTR ~ ., data = train_data)[, -1]
x_train_scaled <- scale(x_train)

# Prepare the data for the test set
x_test <- model.matrix(FTR ~ ., data = test_data)[, -1]

# Align columns between training and test sets
common_columns <- intersect(colnames(x_train), colnames(x_test))
x_test <- x_test[, common_columns, drop = FALSE]                  
x_train <- x_train[, common_columns, drop = FALSE]                

# Rescale training data
x_train_scaled <- scale(x_train)

# Scale the test set using training set scaling parameters
x_test_scaled <- scale(x_test, 
                       center = attr(x_train_scaled, "scaled:center"), 
                       scale = attr(x_train_scaled, "scaled:scale"))

# Fit LASSO regression on the training set
model_fit.lasso <- glmnet(x_train_scaled, train_data$FTR, 
                          family = "multinomial", alpha = 1)

# Perform cross-validation to select lambda
modelcv.lasso <- cv.glmnet(x_train_scaled, train_data$FTR, 
                           family = "multinomial", alpha = 1)

# Get the best lambda (lambda.1se)
model_bestlam.lasso <- modelcv.lasso$lambda.1se

# Predict probabilities for the test set
pred_prob_lasso <- predict(model_fit.lasso, 
                           newx = x_test_scaled, 
                           s = model_bestlam.lasso, 
                           type = "response")

# Predict class labels for the test set
pred_class_lasso <- predict(model_fit.lasso, 
                            newx = x_test_scaled, 
                            s = model_bestlam.lasso, 
                            type = "class")

# Convert predictions to factors with the same levels as the test response variable
pred_class_lasso <- factor(pred_class_lasso, levels = levels(test_data$FTR))

```

Now, let's see the confusion matrix.

```{r}
# Generate confusion matrix for the LASSO model
conf_matrix_lasso <- confusionMatrix(pred_class_lasso, test_data$FTR)

# Print the confusion matrix
print(conf_matrix_lasso)
```

Then we apply the roc function to obtain the best threshold value.

```{r}
# Generate confusion matrix for the LASSO model
conf_matrix_lasso <- confusionMatrix(pred_class_lasso, test_data$FTR)

# Print the confusion matrix
print("Confusion Matrix for LASSO Model:")
print(conf_matrix_lasso)

# Check the structure of predicted probabilities
if (!is.data.frame(pred_prob_lasso)) {
  pred_prob_lasso <- as.data.frame(pred_prob_lasso)
}
```


```{r}
colnames(pred_prob_lasso) <- levels(test_data$FTR)

test_labels <- model.matrix(~ test_data$FTR - 1)
colnames(test_labels) <- levels(test_data$FTR)

plot_roc(test_data, pred_prob_lasso, "LASSO Model")

```

```{r}
# Extract metrics for each class from the LASSO confusion matrix
specificity_awaywin_lasso <- conf_matrix_lasso$byClass["Class: Away Win", "Specificity"]
sensitivity_awaywin_lasso <- conf_matrix_lasso$byClass["Class: Away Win", "Sensitivity"]

specificity_draw_lasso <- conf_matrix_lasso$byClass["Class: Draw", "Specificity"]
sensitivity_draw_lasso <- conf_matrix_lasso$byClass["Class: Draw", "Sensitivity"]

specificity_homewin_lasso <- conf_matrix_lasso$byClass["Class: Home Win", "Specificity"]
sensitivity_homewin_lasso <- conf_matrix_lasso$byClass["Class: Home Win", "Sensitivity"]

accuracy_lasso <- conf_matrix_lasso$overall["Accuracy"]

# Calculate overall metrics for the LASSO model
overall_sensitivity_lasso <- mean(c(sensitivity_awaywin_lasso, sensitivity_draw_lasso, sensitivity_homewin_lasso))
overall_specificity_lasso <- mean(c(specificity_awaywin_lasso, specificity_draw_lasso, specificity_homewin_lasso))
overall_accuracy_lasso <- accuracy_lasso

```

```{r}
# Combine LASSO metrics into a data frame
metrics_lasso_overall <- data.frame(
  Model = "LASSO Model",
  Accuracy = overall_accuracy_lasso,
  Sensitivity = overall_sensitivity_lasso,
  Specificity = overall_specificity_lasso
)

# Add LASSO metrics to the comparison data frame
comparison_df <- rbind(
  comparison_df,
  metrics_lasso_overall
)

# Print the updated comparison
print(comparison_df)
```

### Ridge logistic regression

Now we try with ridge regression. As before we use as starting point the formula with all predictors and interaction term.

```{r}
# Convert predictors to a matrix
x <- model.matrix(FTR ~ ., data = train_data)[, -1] 

# Response variable
y <- train_data$FTR

# Fit ridge regression model
model_fit.ridge <- glmnet(
  x, 
  y, 
  family = "multinomial", 
  alpha = 0
)

# Plot the ridge model
plot(model_fit.ridge, xvar = "lambda", label = TRUE)
```

Choose the best value of lambda via cross-validation.

```{r}
modelcv.ridge <- cv.glmnet(
  x, 
  y, 
  alpha = 0,  
  family = "multinomial"  
)

# Plot cross-validation results
plot(modelcv.ridge)
```

The value of λ that minimizes the ridge cross-validated mean square error is:

```{r}
modelcv.ridge$lambda.min
```

However, we select the simplest model whose λ value is within one standard error from the minimum of the cross-validated mean square error:

```{r}
modelcv_bestlam.ridge <- modelcv.ridge$lambda.1se
modelcv_bestlam.ridge
```

Now visualize again the ridge estimates as a function of the logarithm of λ and add a vertical line corresponding to best.lambda:

```{r}
plot(model_fit.ridge, xvar = "lambda", label = TRUE)
abline(v = log(modelcv_bestlam.ridge), lwd = 1.2, lty = "dashed")
```

These are the coefficients:

```{r}
# Extract coefficients at the best lambda
model_bestlam.ridge.coef <- coef(model_fit.ridge, s = lambda_1se)

# Process coefficients for each class
nonzero_coefs <- lapply(model_bestlam.ridge.coef, function(coef_matrix) {
  coef_matrix <- as.matrix(coef_matrix)
  coef_matrix[coef_matrix != 0, , drop = FALSE]
})

print(nonzero_coefs)
```

We make the prediction on the test set.

```{r}
# Extract coefficients at the best lambda
model_bestlam.ridge.coef <- coef(model_fit.ridge, s = lambda_1se)

# Process coefficients for each class
nonzero_coefs <- lapply(model_bestlam.ridge.coef, function(coef_matrix) {
  coef_matrix <- as.matrix(coef_matrix)
  coef_matrix[coef_matrix != 0, , drop = FALSE]
})

print(nonzero_coefs)
```

```{r}
# Prepare the data for the training set
x_train <- model.matrix(FTR ~ ., data = train_data)[, -1]
x_train_scaled <- scale(x_train)

# Prepare the data for the test set
x_test <- model.matrix(FTR ~ ., data = test_data)[, -1]

# Align columns between training and test sets
common_columns <- intersect(colnames(x_train), colnames(x_test))
x_test <- x_test[, common_columns, drop = FALSE]                  
x_train <- x_train[, common_columns, drop = FALSE]                

# Rescale training data
x_train_scaled <- scale(x_train)

# Scale the test set using training set scaling parameters
x_test_scaled <- scale(x_test, 
                       center = attr(x_train_scaled, "scaled:center"), 
                       scale = attr(x_train_scaled, "scaled:scale"))

# Fit Ridge regression on the training set
model_fit.ridge <- glmnet(x_train_scaled, train_data$FTR, 
                          family = "multinomial", alpha = 0)

# Perform cross-validation to select lambda
modelcv.ridge <- cv.glmnet(x_train_scaled, train_data$FTR, 
                           family = "multinomial", alpha = 0)

# Get the best lambda (lambda.1se)
model_bestlam.ridge <- modelcv.ridge$lambda.1se

# Make predictions
pred_class_ridge <- predict(model_fit.ridge, newx = x_test_scaled, s = model_bestlam.ridge, type = "class")

# Check predictions
cat("Predicted classes:\n")
print(table(pred_class_ridge))

# Ensure both variables are factors with matching levels
test_data$FTR <- factor(test_data$FTR)
pred_class_ridge <- factor(pred_class_ridge, levels = levels(test_data$FTR))

# Verify overlap
cat("\nOverlap in levels:\n")
print(intersect(levels(pred_class_ridge), levels(test_data$FTR)))

# Generate the confusion matrix
conf_matrix_ridge <- confusionMatrix(pred_class_ridge, test_data$FTR)

# Print the confusion matrix
print(conf_matrix_ridge)

```

```{r}
# Get predicted probabilities for Ridge model
pred_prob_ridge <- predict(model_fit.ridge, 
                           newx = x_test_scaled, 
                           s = model_bestlam.ridge, 
                           type = "response")

pred_prob_ridge <- matrix(pred_prob_ridge, ncol = length(levels(test_data$FTR)))

# Ensure the column names match the levels of the response variable
colnames(pred_prob_ridge) <- levels(test_data$FTR)

# Generate true labels for ROC computation
test_labels <- model.matrix(~ test_data$FTR - 1)
colnames(test_labels) <- levels(test_data$FTR)

# Plot the ROC curve for the Ridge model
plot_roc(test_data, pred_prob_ridge, "Ridge Model")
```

```{r}
# Extract metrics for each class from the Ridge confusion matrix
specificity_awaywin_ridge <- conf_matrix_ridge$byClass["Class: Away Win", "Specificity"]
sensitivity_awaywin_ridge <- conf_matrix_ridge$byClass["Class: Away Win", "Sensitivity"]

specificity_draw_ridge <- conf_matrix_ridge$byClass["Class: Draw", "Specificity"]
sensitivity_draw_ridge <- conf_matrix_ridge$byClass["Class: Draw", "Sensitivity"]

specificity_homewin_ridge <- conf_matrix_ridge$byClass["Class: Home Win", "Specificity"]
sensitivity_homewin_ridge <- conf_matrix_ridge$byClass["Class: Home Win", "Sensitivity"]

accuracy_ridge <- conf_matrix_ridge$overall["Accuracy"]

# Calculate overall metrics for the Ridge model
overall_sensitivity_ridge <- mean(c(sensitivity_awaywin_ridge, sensitivity_draw_ridge, sensitivity_homewin_ridge))
overall_specificity_ridge <- mean(c(specificity_awaywin_ridge, specificity_draw_ridge, specificity_homewin_ridge))
overall_accuracy_ridge <- accuracy_ridge
```

```{r}
# Combine Ridge metrics into a data frame
metrics_ridge_overall <- data.frame(
  Model = "Ridge Model",
  Accuracy = overall_accuracy_ridge,
  Sensitivity = overall_sensitivity_ridge,
  Specificity = overall_specificity_ridge
)

# Add Ridge metrics to the comparison data frame
comparison_df <- rbind(
  comparison_df,
  metrics_ridge_overall
)

# Print the updated comparison
print(comparison_df)
```

## Naive Bayes

Continuing with the analysis we try Naive Bayes algorithm. In this case we can’t use a model with interaction term since Naive Bayes do not support these relations. Thus we decide to use the Full Model. 

```{r}
model_nb <- naiveBayes(FTR ~ ., data = train_data)
model_nb
```

Prediction for the test set:

```{r}
# Get predicted probabilities for Naive Bayes model
pred_prob_nb <- predict(model_nb, newdata = test_data, type = "raw")

```

Let's plot the ROC :

```{r}
# Ensure the column names match the levels of the response variable
colnames(pred_prob_nb) <- levels(test_data$FTR)

# Generate true labels for ROC computation
test_labels <- model.matrix(~ test_data$FTR - 1)
colnames(test_labels) <- levels(test_data$FTR)

# Plot the ROC curve for the Naive Bayes model
plot_roc(test_data, pred_prob_nb, "Naive Bayes Model")
```

```{r}
# Get class predictions for the test set
pred_class_nb <- predict(model_nb, newdata = test_data)

# Ensure both variables are factors with matching levels
test_data$FTR <- factor(test_data$FTR)
pred_class_nb <- factor(pred_class_nb, levels = levels(test_data$FTR))

# Generate confusion matrix
conf_matrix_nb <- confusionMatrix(pred_class_nb, test_data$FTR)

# Print the confusion matrix
print(conf_matrix_nb)
```

```{r}
# Extract metrics for each class from the Naive Bayes confusion matrix
specificity_awaywin_nb <- conf_matrix_nb$byClass["Class: Away Win", "Specificity"]
sensitivity_awaywin_nb <- conf_matrix_nb$byClass["Class: Away Win", "Sensitivity"]

specificity_draw_nb <- conf_matrix_nb$byClass["Class: Draw", "Specificity"]
sensitivity_draw_nb <- conf_matrix_nb$byClass["Class: Draw", "Sensitivity"]

specificity_homewin_nb <- conf_matrix_nb$byClass["Class: Home Win", "Specificity"]
sensitivity_homewin_nb <- conf_matrix_nb$byClass["Class: Home Win", "Sensitivity"]

accuracy_nb <- conf_matrix_nb$overall["Accuracy"]

# Calculate overall metrics for the Naive Bayes model
overall_sensitivity_nb <- mean(c(sensitivity_awaywin_nb, sensitivity_draw_nb, sensitivity_homewin_nb))
overall_specificity_nb <- mean(c(specificity_awaywin_nb, specificity_draw_nb, specificity_homewin_nb))
overall_accuracy_nb <- accuracy_nb
```

```{r}
# Combine Naive Bayes metrics into a data frame
metrics_nb_overall <- data.frame(
  Model = "Naive Bayes",
  Accuracy = overall_accuracy_nb,
  Sensitivity = overall_sensitivity_nb,
  Specificity = overall_specificity_nb
)

# Add Naive Bayes metrics to the comparison data frame
comparison_df <- rbind(
  comparison_df,
  metrics_nb_overall
)

```

## Conclusion

```{r}
print(comparison_df)
```

Let’s find the best model for all the metrics:

1. Accuracy

```{r}
best_accuracy <- comparison_df[which.max(comparison_df$Accuracy), ]
print(best_accuracy)
```

The best model in terms of accuracy is “Full Model”.

2. Sensitivity

```{r}
best_sensitivity <- comparison_df[which.max(comparison_df$Sensitivity), ]
print(best_sensitivity)
```

The best model in terms of sensitivity is “Full Interaction Model”.


3. Specificity

```{r}
best_specificity <- comparison_df[which.max(comparison_df$Specificity), ]
print(best_specificity)
```

The best model in terms of specificity is “Stepwise Model”.

However, recalling the unbalanced problem, our choice of the top three overall models is as follows:

1. **Full Model**: This model has a good balance of accuracy and sensitivity, making it effective at identifying positive cases while maintaining reasonable specificity.

2. **Full Interaction Model**: This model has the highest sensitivity, which is crucial for unbalanced datasets where identifying the minority class is often more important than overall accuracy.

3. **Cleaned Model**: This model provides a good balance between sensitivity and specificity, making it a strong candidate for scenarios where both false positives and false negatives need to be minimized.
